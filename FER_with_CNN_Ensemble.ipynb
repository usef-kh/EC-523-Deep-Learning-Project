{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWy+3kUSe4zk9r6beYlz4h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usef-kh/EC523-Deep-Learning-Project/blob/master/FER_with_CNN_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnO2nvqpALPj"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cWGQl78U9wy",
        "outputId": "f0b1c0c5-4104-4067-923d-2a194a5f589c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone \"https://github.com/usef-kh/EC523-Deep-Learning-Project.git\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EC523-Deep-Learning-Project'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 33 (delta 11), reused 10 (delta 3), pack-reused 8\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgKnn2ux6Pgp",
        "outputId": "dc8afef3-070a-4bf0-e7d3-0dea3d80ef52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install unrar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unrar\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/0b/53130ccd483e3db8c8a460cb579bdb21b458d5494d67a261e1a5b273fbb9/unrar-0.4-py3-none-any.whl\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtrv6tM-Xq2n",
        "outputId": "820d828d-788d-4ef5-8564-0f8261f7d071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# !unrar x \"EC523-Deep-Learning-Project/datasets/ckplus.rar\"\n",
        "!unrar e \"EC523-Deep-Learning-Project/datasets/fer2013.rar\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from EC523-Deep-Learning-Project/datasets/fer2013.rar\n",
            "\n",
            "Extracting  fer2013.csv                                                  \b\b\b\b  4%\b\b\b\b  9%\b\b\b\b 13%\b\b\b\b 18%\b\b\b\b 23%\b\b\b\b 27%\b\b\b\b 32%\b\b\b\b 37%\b\b\b\b 41%\b\b\b\b 46%\b\b\b\b 51%\b\b\b\b 55%\b\b\b\b 60%\b\b\b\b 65%\b\b\b\b 69%\b\b\b\b 74%\b\b\b\b 79%\b\b\b\b 83%\b\b\b\b 88%\b\b\b\b 93%\b\b\b\b 97%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN_6jQNFAH8t"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ-kmOyAAHLn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBi_q71MAOLY"
      },
      "source": [
        "##Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYuiGVdr_3PF",
        "outputId": "156d67e8-7094-4518-ddb6-f5f4507117c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "fer2013 = pd.read_csv('fer2013.csv') \n",
        "emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
        "fer2013.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F54zA1oAASTM"
      },
      "source": [
        "def prepare_data(data):\n",
        "    \"\"\" Prepare data for modeling \n",
        "        input: data frame with labels und pixel data\n",
        "        output: image and label array \"\"\"\n",
        "    \n",
        "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
        "    image_label = np.array(list(map(int, data['emotion'])))\n",
        "    \n",
        "    for i, row in enumerate(data.index):\n",
        "        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n",
        "        image = np.reshape(image, (48, 48))\n",
        "        image_array[i] = image\n",
        "        \n",
        "    return image_array, image_label\n",
        "\n",
        "\n",
        "def reformat_data(X, Y):\n",
        "    data = []\n",
        "\n",
        "    for x, y in zip(torch.from_numpy(X), torch.from_numpy(Y)):\n",
        "        x, y = x.type(torch.DoubleTensor), y.type(torch.long)\n",
        "        data.append((x.unsqueeze(0), y))\n",
        "\n",
        "    return data\n",
        "\n",
        "xtrain, ytrain = prepare_data(fer2013[fer2013['Usage'] == 'Training'])\n",
        "xval , yval = prepare_data(fer2013[fer2013['Usage'] == 'PrivateTest'])\n",
        "xtest, ytest = prepare_data(fer2013[fer2013['Usage'] == 'PublicTest'])\n",
        "\n",
        "train = reformat_data(xtrain, ytrain)\n",
        "val = reformat_data(xval, yval)\n",
        "test = reformat_data(xtest, ytest)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size=100, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(val, batch_size=100, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "del train, test, val, xtrain, ytrain, xval, yval, xtest, ytest, fer2013"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lv4xMhpGOBu",
        "outputId": "3b02d07d-4c6b-42f6-a023-d53352d5c494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# # get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "print(labels.shape)\n",
        "\n",
        "# print(images.shape)\n",
        "# img = images[0]\n",
        "# print(img.shape)\n",
        "# plt.imshow(img.numpy())\n",
        "# print(emotions[labels[0].item()])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIEFhmKoCyZ1"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV0nfy_5CzwG"
      },
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class Subnet1(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Subnet1, self).__init__()\n",
        "#         # Not sure about number of in channels, may have to change!\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # according to paper!\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "#         self.conv2 = nn.Conv2d(64,128,3,padding=1)\n",
        "#         self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "\n",
        "#         self.lin1 = nn.Linear(128 * 10 * 10, 4096) # Will have to change input size\n",
        "#         self.lin2 = nn.Linear(4096, 4096)\n",
        "#         self.lin3 = nn.Linear(4096, 7)\n",
        "\n",
        "#         #self.drop = nn.Dropout(0.2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         #print(\"shape after 1 conv layer: \", x.shape)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         #print(\"shape after 2 conv layer: \", x.shape)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         #print(\"shape after 3 conv layer: \", x.shape)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         #print(\"shape before linear layers!!: \", x.shape)\n",
        "\n",
        "#         x = x.view(x.size(0), 128*10*10) # will have to change!\n",
        "\n",
        "#         x = F.relu(self.lin1(x))\n",
        "#         x = F.relu(self.lin2(x))\n",
        "#         x = self.lin3(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# sub1 = Subnet1()\n",
        "\n",
        "\n",
        "# #####################################\n",
        "\n",
        "# # define nets for CNN emotion detection problem\n",
        "\n",
        "# class Subnet2(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Subnet2, self).__init__()\n",
        "#         # Not sure about number of in channels, may have to change!\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # according to paper!\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "#         self.conv2 = nn.Conv2d(64,128,3,padding=1)\n",
        "#         self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "#         self.conv3_2 = nn.Conv2d(256,256, 3, padding=1)\n",
        "#         self.lin1 = nn.Linear(128 * 10 * 10, 4096) # MUST CHANGE\n",
        "#         self.lin2 = nn.Linear(4096, 4096)\n",
        "#         self.lin3 = nn.Linear(4096, 7)\n",
        "\n",
        "#         #self.drop = nn.Dropout(0.2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         #print(\"shape after 1 conv layer: \", x.shape)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         #print(\"shape after 2 conv layer: \", x.shape)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.relu(self.conv3_2(x))\n",
        "#         #print(\"shape after 3 conv layer: \", x.shape)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         x = x.view(x.size(0), 128*10*10)\n",
        "\n",
        "#         x = F.relu(self.lin1(x))\n",
        "#         x = F.relu(self.lin2(x))\n",
        "#         x = self.lin3(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# sub2 = Subnet2()\n",
        "\n",
        "# # define nets for CNN emotion detection problem\n",
        "\n",
        "# class Subnet3(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Subnet3, self).__init__()\n",
        "#         # Not sure about number of in channels, may have to change!\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # according to paper!\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "#         self.conv2 = nn.Conv2d(64,128,3,padding=1)\n",
        "#         self.conv2_2 = nn.Conv2d(128,128,3,padding=1)\n",
        "#         self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "#         self.conv3_2 = nn.Conv2d(256,256, 3, padding=1)\n",
        "#         self.lin1 = nn.Linear(128 * 10 * 10, 4096) # MUST CHANGE\n",
        "#         self.lin2 = nn.Linear(4096, 4096)\n",
        "#         self.lin3 = nn.Linear(4096, 7)\n",
        "\n",
        "#         #self.drop = nn.Dropout(0.2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         #print(\"shape after 1 conv layer: \", x.shape)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.relu(self.conv2_2(x))\n",
        "#         #print(\"shape after 2 conv layer: \", x.shape)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.relu(self.conv3_2(x))\n",
        "#         #print(\"shape after 3 conv layer: \", x.shape)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         x = x.view(x.size(0), 128*10*10) # CHANGE\n",
        "\n",
        "#         x = F.relu(self.lin1(x))\n",
        "#         x = F.relu(self.lin2(x))\n",
        "#         x = self.lin3(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# sub3 = Subnet3()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCuMQu5bF5nE"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVUpKgfDIYt"
      },
      "source": [
        "class Subnet1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Subnet1, self).__init__()\n",
        "        # Not sure about number of in channels, may have to change!\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # according to paper!\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.conv2 = nn.Conv2d(64,128,3,padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "\n",
        "        self.lin1 = nn.Linear(256*6*6, 4096) # Will have to change input size\n",
        "        self.lin2 = nn.Linear(4096, 4096)\n",
        "        self.lin3 = nn.Linear(4096, 7)\n",
        "        \n",
        "        #self.drop = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(\"shape after 1 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        #print(\"shape after 2 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        #print(\"shape after 3 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # print(\"shape before linear layers!!: \", x.shape)\n",
        "\n",
        "        x = x.view(-1, 256*6*6) # will have to change!\n",
        "        # print(\"reshaped\", x.shape)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "sub1 = Subnet1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wME97MptDeex",
        "outputId": "344fb6fd-f3e7-450c-ad90-c9c6558b97d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "net = Subnet1()\n",
        "net = net.to(device)\n",
        "net.double()zoom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Subnet1(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (lin1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "  (lin2): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (lin3): Linear(in_features=4096, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACcirLVUPzUd"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3asDIi_yDQLW"
      },
      "source": [
        "def train_model(net, trainloader, valloader):\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    num = 10\n",
        "    for epoch in range(20):\n",
        "        print(\"Training\")\n",
        "        net = net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % num == num - 1 :    \n",
        "                print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / num))\n",
        "                train_loss.append(running_loss / num)\n",
        "                running_loss = 0.0\n",
        "        \n",
        "        print(\"Validating\")\n",
        "        net = net.eval()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(valloader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % num == num - 1:    \n",
        "                print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / num))\n",
        "                val_loss.append(running_loss / num)\n",
        "                running_loss = 0.0\n",
        "\n",
        "    return train_loss, val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9npmfIq5FgZ1",
        "outputId": "a084d42b-d4bd-455e-c5eb-5a163f70d479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss, val_loss = train_model(net, trainloader, valloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training\n",
            "[1,    10] loss: 8.412\n",
            "[1,    20] loss: 1.878\n",
            "[1,    30] loss: 1.857\n",
            "[1,    40] loss: 1.834\n",
            "[1,    50] loss: 1.811\n",
            "[1,    60] loss: 1.807\n",
            "[1,    70] loss: 1.791\n",
            "[1,    80] loss: 1.769\n",
            "[1,    90] loss: 1.766\n",
            "[1,   100] loss: 1.768\n",
            "[1,   110] loss: 1.747\n",
            "[1,   120] loss: 1.747\n",
            "[1,   130] loss: 1.743\n",
            "[1,   140] loss: 1.792\n",
            "[1,   150] loss: 1.745\n",
            "[1,   160] loss: 1.721\n",
            "[1,   170] loss: 1.724\n",
            "[1,   180] loss: 1.737\n",
            "[1,   190] loss: 1.728\n",
            "[1,   200] loss: 1.713\n",
            "[1,   210] loss: 1.690\n",
            "[1,   220] loss: 1.668\n",
            "[1,   230] loss: 1.692\n",
            "[1,   240] loss: 1.642\n",
            "[1,   250] loss: 1.664\n",
            "[1,   260] loss: 1.675\n",
            "[1,   270] loss: 1.614\n",
            "[1,   280] loss: 1.533\n",
            "Validating\n",
            "[1,    10] loss: 1.628\n",
            "[1,    20] loss: 1.619\n",
            "[1,    30] loss: 1.609\n",
            "Training\n",
            "[2,    10] loss: 1.627\n",
            "[2,    20] loss: 1.590\n",
            "[2,    30] loss: 1.636\n",
            "[2,    40] loss: 1.615\n",
            "[2,    50] loss: 1.649\n",
            "[2,    60] loss: 1.634\n",
            "[2,    70] loss: 1.576\n",
            "[2,    80] loss: 1.546\n",
            "[2,    90] loss: 1.595\n",
            "[2,   100] loss: 1.558\n",
            "[2,   110] loss: 1.553\n",
            "[2,   120] loss: 1.506\n",
            "[2,   130] loss: 1.564\n",
            "[2,   140] loss: 1.567\n",
            "[2,   150] loss: 1.532\n",
            "[2,   160] loss: 1.489\n",
            "[2,   170] loss: 1.534\n",
            "[2,   180] loss: 1.517\n",
            "[2,   190] loss: 1.533\n",
            "[2,   200] loss: 1.527\n",
            "[2,   210] loss: 1.480\n",
            "[2,   220] loss: 1.544\n",
            "[2,   230] loss: 1.496\n",
            "[2,   240] loss: 1.494\n",
            "[2,   250] loss: 1.450\n",
            "[2,   260] loss: 1.462\n",
            "[2,   270] loss: 1.480\n",
            "[2,   280] loss: 1.485\n",
            "Validating\n",
            "[2,    10] loss: 1.498\n",
            "[2,    20] loss: 1.456\n",
            "[2,    30] loss: 1.498\n",
            "Training\n",
            "[3,    10] loss: 1.428\n",
            "[3,    20] loss: 1.393\n",
            "[3,    30] loss: 1.429\n",
            "[3,    40] loss: 1.406\n",
            "[3,    50] loss: 1.437\n",
            "[3,    60] loss: 1.399\n",
            "[3,    70] loss: 1.426\n",
            "[3,    80] loss: 1.343\n",
            "[3,    90] loss: 1.428\n",
            "[3,   100] loss: 1.409\n",
            "[3,   110] loss: 1.398\n",
            "[3,   120] loss: 1.354\n",
            "[3,   130] loss: 1.386\n",
            "[3,   140] loss: 1.409\n",
            "[3,   150] loss: 1.413\n",
            "[3,   160] loss: 1.441\n",
            "[3,   170] loss: 1.433\n",
            "[3,   180] loss: 1.408\n",
            "[3,   190] loss: 1.382\n",
            "[3,   200] loss: 1.356\n",
            "[3,   210] loss: 1.396\n",
            "[3,   220] loss: 1.391\n",
            "[3,   230] loss: 1.439\n",
            "[3,   240] loss: 1.363\n",
            "[3,   250] loss: 1.330\n",
            "[3,   260] loss: 1.350\n",
            "[3,   270] loss: 1.323\n",
            "[3,   280] loss: 1.375\n",
            "Validating\n",
            "[3,    10] loss: 1.354\n",
            "[3,    20] loss: 1.342\n",
            "[3,    30] loss: 1.327\n",
            "Training\n",
            "[4,    10] loss: 1.271\n",
            "[4,    20] loss: 1.265\n",
            "[4,    30] loss: 1.272\n",
            "[4,    40] loss: 1.265\n",
            "[4,    50] loss: 1.288\n",
            "[4,    60] loss: 1.303\n",
            "[4,    70] loss: 1.296\n",
            "[4,    80] loss: 1.254\n",
            "[4,    90] loss: 1.267\n",
            "[4,   100] loss: 1.261\n",
            "[4,   110] loss: 1.268\n",
            "[4,   120] loss: 1.262\n",
            "[4,   130] loss: 1.291\n",
            "[4,   140] loss: 1.304\n",
            "[4,   150] loss: 1.339\n",
            "[4,   160] loss: 1.267\n",
            "[4,   170] loss: 1.322\n",
            "[4,   180] loss: 1.227\n",
            "[4,   190] loss: 1.251\n",
            "[4,   200] loss: 1.251\n",
            "[4,   210] loss: 1.239\n",
            "[4,   220] loss: 1.256\n",
            "[4,   230] loss: 1.182\n",
            "[4,   240] loss: 1.221\n",
            "[4,   250] loss: 1.324\n",
            "[4,   260] loss: 1.309\n",
            "[4,   270] loss: 1.247\n",
            "[4,   280] loss: 1.302\n",
            "Validating\n",
            "[4,    10] loss: 1.297\n",
            "[4,    20] loss: 1.304\n",
            "[4,    30] loss: 1.357\n",
            "Training\n",
            "[5,    10] loss: 1.201\n",
            "[5,    20] loss: 1.146\n",
            "[5,    30] loss: 1.136\n",
            "[5,    40] loss: 1.128\n",
            "[5,    50] loss: 1.108\n",
            "[5,    60] loss: 1.177\n",
            "[5,    70] loss: 1.165\n",
            "[5,    80] loss: 1.133\n",
            "[5,    90] loss: 1.159\n",
            "[5,   100] loss: 1.128\n",
            "[5,   110] loss: 1.179\n",
            "[5,   120] loss: 1.188\n",
            "[5,   130] loss: 1.139\n",
            "[5,   140] loss: 1.178\n",
            "[5,   150] loss: 1.150\n",
            "[5,   160] loss: 1.157\n",
            "[5,   170] loss: 1.156\n",
            "[5,   180] loss: 1.137\n",
            "[5,   190] loss: 1.181\n",
            "[5,   200] loss: 1.148\n",
            "[5,   210] loss: 1.152\n",
            "[5,   220] loss: 1.173\n",
            "[5,   230] loss: 1.163\n",
            "[5,   240] loss: 1.162\n",
            "[5,   250] loss: 1.139\n",
            "[5,   260] loss: 1.110\n",
            "[5,   270] loss: 1.161\n",
            "[5,   280] loss: 1.076\n",
            "Validating\n",
            "[5,    10] loss: 1.213\n",
            "[5,    20] loss: 1.244\n",
            "[5,    30] loss: 1.255\n",
            "Training\n",
            "[6,    10] loss: 0.967\n",
            "[6,    20] loss: 1.006\n",
            "[6,    30] loss: 1.002\n",
            "[6,    40] loss: 0.991\n",
            "[6,    50] loss: 0.968\n",
            "[6,    60] loss: 0.957\n",
            "[6,    70] loss: 1.032\n",
            "[6,    80] loss: 0.956\n",
            "[6,    90] loss: 1.072\n",
            "[6,   100] loss: 0.977\n",
            "[6,   110] loss: 1.001\n",
            "[6,   120] loss: 0.966\n",
            "[6,   130] loss: 0.995\n",
            "[6,   140] loss: 1.028\n",
            "[6,   150] loss: 0.999\n",
            "[6,   160] loss: 1.052\n",
            "[6,   170] loss: 1.046\n",
            "[6,   180] loss: 0.976\n",
            "[6,   190] loss: 1.031\n",
            "[6,   200] loss: 1.011\n",
            "[6,   210] loss: 0.963\n",
            "[6,   220] loss: 0.990\n",
            "[6,   230] loss: 1.056\n",
            "[6,   240] loss: 1.065\n",
            "[6,   250] loss: 1.000\n",
            "[6,   260] loss: 1.068\n",
            "[6,   270] loss: 1.061\n",
            "[6,   280] loss: 1.004\n",
            "Validating\n",
            "[6,    10] loss: 1.429\n",
            "[6,    20] loss: 1.320\n",
            "[6,    30] loss: 1.351\n",
            "Training\n",
            "[7,    10] loss: 0.878\n",
            "[7,    20] loss: 0.781\n",
            "[7,    30] loss: 0.818\n",
            "[7,    40] loss: 0.754\n",
            "[7,    50] loss: 0.844\n",
            "[7,    60] loss: 0.774\n",
            "[7,    70] loss: 0.801\n",
            "[7,    80] loss: 0.822\n",
            "[7,    90] loss: 0.789\n",
            "[7,   100] loss: 0.757\n",
            "[7,   110] loss: 0.836\n",
            "[7,   120] loss: 0.810\n",
            "[7,   130] loss: 0.818\n",
            "[7,   140] loss: 0.823\n",
            "[7,   150] loss: 0.858\n",
            "[7,   160] loss: 0.895\n",
            "[7,   170] loss: 0.843\n",
            "[7,   180] loss: 0.824\n",
            "[7,   190] loss: 0.846\n",
            "[7,   200] loss: 0.843\n",
            "[7,   210] loss: 0.913\n",
            "[7,   220] loss: 0.937\n",
            "[7,   230] loss: 0.932\n",
            "[7,   240] loss: 0.943\n",
            "[7,   250] loss: 0.851\n",
            "[7,   260] loss: 0.867\n",
            "[7,   270] loss: 0.892\n",
            "[7,   280] loss: 0.934\n",
            "Validating\n",
            "[7,    10] loss: 1.437\n",
            "[7,    20] loss: 1.351\n",
            "[7,    30] loss: 1.305\n",
            "Training\n",
            "[8,    10] loss: 0.683\n",
            "[8,    20] loss: 0.592\n",
            "[8,    30] loss: 0.599\n",
            "[8,    40] loss: 0.556\n",
            "[8,    50] loss: 0.549\n",
            "[8,    60] loss: 0.611\n",
            "[8,    70] loss: 0.689\n",
            "[8,    80] loss: 0.584\n",
            "[8,    90] loss: 0.665\n",
            "[8,   100] loss: 0.642\n",
            "[8,   110] loss: 0.681\n",
            "[8,   120] loss: 0.669\n",
            "[8,   130] loss: 0.692\n",
            "[8,   140] loss: 0.656\n",
            "[8,   150] loss: 0.719\n",
            "[8,   160] loss: 0.661\n",
            "[8,   170] loss: 0.685\n",
            "[8,   180] loss: 0.695\n",
            "[8,   190] loss: 0.668\n",
            "[8,   200] loss: 0.690\n",
            "[8,   210] loss: 0.718\n",
            "[8,   220] loss: 0.792\n",
            "[8,   230] loss: 0.725\n",
            "[8,   240] loss: 0.735\n",
            "[8,   250] loss: 0.697\n",
            "[8,   260] loss: 0.684\n",
            "[8,   270] loss: 0.744\n",
            "[8,   280] loss: 0.804\n",
            "Validating\n",
            "[8,    10] loss: 1.551\n",
            "[8,    20] loss: 1.569\n",
            "[8,    30] loss: 1.429\n",
            "Training\n",
            "[9,    10] loss: 0.482\n",
            "[9,    20] loss: 0.458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaGNnlXTJdeG"
      },
      "source": [
        "fig, ax = plt.subplots()  # Create a figure and an axes.\n",
        "ax.plot(train_loss, 'b')  # ... and some more.\n",
        "ax.set_xlabel('Iteration(batch)')  # Add an x-label to the axes.\n",
        "ax.set_ylabel('Training Loss')  # Add a y-label to the axes.\n",
        "ax.set_title(\"Loss vs. Iteration\")  # Add a title to the axes.\n",
        "\n",
        "fig, ax = plt.subplots()  # Create a figure and an axes.\n",
        "ax.plot(val_loss, 'r')  # ... and some more.\n",
        "ax.set_xlabel('Iteration(batch)')  # Add an x-label to the axes.\n",
        "ax.set_ylabel('Validation Loss')  # Add a y-label to the axes.\n",
        "ax.set_title(\"Loss vs. Iteration\")  # Add a title to the axes.\n",
        "\n",
        "\n",
        "# plt.plot(train_loss)\n",
        "# plt.plot(val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}