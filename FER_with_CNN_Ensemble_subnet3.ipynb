{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FER with CNN Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnO2nvqpALPj"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cWGQl78U9wy",
        "outputId": "5660ffd6-2814-48a2-bf9e-e3cf17afa385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!git clone \"https://github.com/usef-kh/EC523-Deep-Learning-Project.git\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EC523-Deep-Learning-Project'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 27 (delta 8), reused 3 (delta 0), pack-reused 8\u001b[K\n",
            "Unpacking objects: 100% (27/27), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgKnn2ux6Pgp",
        "outputId": "414748d2-94a4-4318-e328-97562567c882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "!pip install unrar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unrar\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/0b/53130ccd483e3db8c8a460cb579bdb21b458d5494d67a261e1a5b273fbb9/unrar-0.4-py3-none-any.whl\n",
            "Installing collected packages: unrar\n",
            "Successfully installed unrar-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtrv6tM-Xq2n",
        "outputId": "f1e5c285-ac7f-4f95-f870-897b3cabb05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# !unrar x \"EC523-Deep-Learning-Project/datasets/ckplus.rar\"\n",
        "!unrar e \"EC523-Deep-Learning-Project/datasets/fer2013.rar\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from EC523-Deep-Learning-Project/datasets/fer2013.rar\n",
            "\n",
            "Extracting  fer2013.csv                                                  \b\b\b\b  4%\b\b\b\b  9%\b\b\b\b 13%\b\b\b\b 18%\b\b\b\b 23%\b\b\b\b 27%\b\b\b\b 32%\b\b\b\b 37%\b\b\b\b 41%\b\b\b\b 46%\b\b\b\b 51%\b\b\b\b 55%\b\b\b\b 60%\b\b\b\b 65%\b\b\b\b 69%\b\b\b\b 74%\b\b\b\b 79%\b\b\b\b 83%\b\b\b\b 88%\b\b\b\b 93%\b\b\b\b 97%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN_6jQNFAH8t"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ-kmOyAAHLn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBi_q71MAOLY"
      },
      "source": [
        "##Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYuiGVdr_3PF",
        "outputId": "5b56d9bf-d29f-453c-f589-5de099146b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "fer2013 = pd.read_csv('fer2013.csv') \n",
        "emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
        "fer2013.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F54zA1oAASTM"
      },
      "source": [
        "def prepare_data(data):\n",
        "    \"\"\" Prepare data for modeling \n",
        "        input: data frame with labels und pixel data\n",
        "        output: image and label array \"\"\"\n",
        "    \n",
        "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
        "    image_label = np.array(list(map(int, data['emotion'])))\n",
        "    \n",
        "    for i, row in enumerate(data.index):\n",
        "        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n",
        "        image = np.reshape(image, (48, 48))\n",
        "        image_array[i] = image\n",
        "        \n",
        "    return image_array, image_label\n",
        "\n",
        "\n",
        "def reformat_data(X, Y):\n",
        "    data = []\n",
        "\n",
        "    for x, y in zip(torch.from_numpy(X), torch.from_numpy(Y)):\n",
        "        x, y = x.type(torch.DoubleTensor), y.type(torch.long)\n",
        "        data.append((x.unsqueeze(0), y))\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "xtrain, ytrain = prepare_data(fer2013[fer2013['Usage'] == 'Training'])\n",
        "xval , yval = prepare_data(fer2013[fer2013['Usage'] == 'PrivateTest'])\n",
        "xtest, ytest = prepare_data(fer2013[fer2013['Usage'] == 'PublicTest'])\n",
        "\n",
        "train = reformat_data(xtrain, ytrain)\n",
        "val = reformat_data(xval, yval)\n",
        "test = reformat_data(xtest, ytest)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train, batch_size=100, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(val, batch_size=100, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "del train, test, val, xtrain, ytrain, xval, yval, xtest, ytest, fer2013"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lv4xMhpGOBu",
        "outputId": "f166494a-0773-4570-f338-d6e6fcf40268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "# # get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "print(labels.shape)\n",
        "\n",
        "# print(images.shape)\n",
        "# img = images[0]\n",
        "# print(img.shape)\n",
        "# plt.imshow(img.numpy())\n",
        "# print(emotions[labels[0].item()])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIEFhmKoCyZ1"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV0nfy_5CzwG"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Subnet1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Subnet1, self).__init__()\n",
        "        # Not sure about number of in channels, may have to change!\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # according to paper!\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.conv2 = nn.Conv2d(64,128,3,padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "\n",
        "        self.lin1 = nn.Linear(128 * 10 * 10, 4096) # Will have to change input size\n",
        "        self.lin2 = nn.Linear(4096, 4096)\n",
        "        self.lin3 = nn.Linear(4096, 7)\n",
        "\n",
        "        #self.drop = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(\"shape after 1 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        #print(\"shape after 2 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        #print(\"shape after 3 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        #print(\"shape before linear layers!!: \", x.shape)\n",
        "\n",
        "        x = x.view(x.size(0), 128*10*10) # will have to change!\n",
        "\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "sub1 = Subnet1()\n",
        "\n",
        "\n",
        "#####################################\n",
        "\n",
        "# define nets for CNN emotion detection problem\n",
        "\n",
        "class Subnet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Subnet2, self).__init__()\n",
        "        # Not sure about number of in channels, may have to change!\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # according to paper!\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.conv2 = nn.Conv2d(64,128,3,padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256,256, 3, padding=1)\n",
        "        self.lin1 = nn.Linear(128 * 10 * 10, 4096) # MUST CHANGE\n",
        "        self.lin2 = nn.Linear(4096, 4096)\n",
        "        self.lin3 = nn.Linear(4096, 7)\n",
        "\n",
        "        #self.drop = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(\"shape after 1 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        #print(\"shape after 2 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        #print(\"shape after 3 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(x.size(0), 128*10*10)\n",
        "\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "sub2 = Subnet2()\n",
        "\n",
        "# define nets for CNN emotion detection problem\n",
        "\n",
        "class Subnet3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Subnet3, self).__init__()\n",
        "        # Not sure about number of in channels, may have to change!\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # according to paper!\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.conv2 = nn.Conv2d(64,128,3,padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128,128,3,padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256,256, 3, padding=1)\n",
        "        self.lin1 = nn.Linear(128 * 10 * 10, 4096) # MUST CHANGE\n",
        "        self.lin2 = nn.Linear(4096, 4096)\n",
        "        self.lin3 = nn.Linear(4096, 7)\n",
        "\n",
        "        #self.drop = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(\"shape after 1 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        #print(\"shape after 2 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        #print(\"shape after 3 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(x.size(0), 128*10*10) # CHANGE\n",
        "\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "sub3 = Subnet3()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCuMQu5bF5nE"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVUpKgfDIYt"
      },
      "source": [
        "class Subnet3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Subnet3, self).__init__()\n",
        "        # Not sure about number of in channels, may have to change!\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1) # according to paper!\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.conv2 = nn.Conv2d(64,128,3,padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128,128,3,padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256,256, 3, padding=1)\n",
        "        self.lin1 = nn.Linear(256 * 6 * 6, 4096) # MUST CHANGE\n",
        "        self.lin2 = nn.Linear(4096, 4096)\n",
        "        self.lin3 = nn.Linear(4096, 7)\n",
        "\n",
        "\n",
        "        #self.drop = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(\"shape after 1 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        #print(\"shape after 2 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        #print(\"shape after 3 conv layer: \", x.shape)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(-1, 256*6*6) # CHANGE\n",
        "      \n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "sub3 = Subnet3()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wME97MptDeex",
        "outputId": "79e7756b-2c58-4690-9de4-ec76861801ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "net = Subnet3()\n",
        "net = net.to(device)\n",
        "net.double()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Subnet3(\n",
              "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (lin1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "  (lin2): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (lin3): Linear(in_features=4096, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACcirLVUPzUd"
      },
      "source": [
        "# criterion = nn.Softmax()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3asDIi_yDQLW"
      },
      "source": [
        "def train_model(net, trainloader, valloader):\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    num = 10\n",
        "    for epoch in range(20):\n",
        "        print(\"Training\")\n",
        "        net = net.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % num == num - 1 :    \n",
        "                print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / num))\n",
        "                train_loss.append(running_loss / num)\n",
        "                running_loss = 0.0\n",
        "        \n",
        "        print(\"Validating\")\n",
        "        net = net.eval()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(valloader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % num == num - 1:    \n",
        "                print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / num))\n",
        "                val_loss.append(running_loss / num)\n",
        "                running_loss = 0.0\n",
        "\n",
        "    return train_loss, val_loss"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9npmfIq5FgZ1",
        "outputId": "a1aaaa43-9845-43bb-aa98-e1b06d34c49e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss, val_loss = train_model(net, trainloader, valloader)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training\n",
            "[1,    10] loss: 1.913\n",
            "[1,    20] loss: 1.839\n",
            "[1,    30] loss: 1.851\n",
            "[1,    40] loss: 1.835\n",
            "[1,    50] loss: 1.814\n",
            "[1,    60] loss: 1.835\n",
            "[1,    70] loss: 1.806\n",
            "[1,    80] loss: 1.811\n",
            "[1,    90] loss: 1.824\n",
            "[1,   100] loss: 1.814\n",
            "[1,   110] loss: 1.795\n",
            "[1,   120] loss: 1.812\n",
            "[1,   130] loss: 1.826\n",
            "[1,   140] loss: 1.817\n",
            "[1,   150] loss: 1.806\n",
            "[1,   160] loss: 1.812\n",
            "[1,   170] loss: 1.798\n",
            "[1,   180] loss: 1.786\n",
            "[1,   190] loss: 1.806\n",
            "[1,   200] loss: 1.762\n",
            "[1,   210] loss: 1.789\n",
            "[1,   220] loss: 1.731\n",
            "[1,   230] loss: 1.743\n",
            "[1,   240] loss: 1.734\n",
            "[1,   250] loss: 1.690\n",
            "[1,   260] loss: 1.635\n",
            "[1,   270] loss: 1.627\n",
            "[1,   280] loss: 1.675\n",
            "Validating\n",
            "[1,    10] loss: 1.765\n",
            "[1,    20] loss: 1.836\n",
            "[1,    30] loss: 1.836\n",
            "Training\n",
            "[2,    10] loss: 1.687\n",
            "[2,    20] loss: 1.610\n",
            "[2,    30] loss: 1.582\n",
            "[2,    40] loss: 1.569\n",
            "[2,    50] loss: 1.598\n",
            "[2,    60] loss: 1.541\n",
            "[2,    70] loss: 1.596\n",
            "[2,    80] loss: 1.561\n",
            "[2,    90] loss: 1.583\n",
            "[2,   100] loss: 1.530\n",
            "[2,   110] loss: 1.466\n",
            "[2,   120] loss: 1.542\n",
            "[2,   130] loss: 1.526\n",
            "[2,   140] loss: 1.561\n",
            "[2,   150] loss: 1.524\n",
            "[2,   160] loss: 1.502\n",
            "[2,   170] loss: 1.458\n",
            "[2,   180] loss: 1.538\n",
            "[2,   190] loss: 1.495\n",
            "[2,   200] loss: 1.454\n",
            "[2,   210] loss: 1.438\n",
            "[2,   220] loss: 1.484\n",
            "[2,   230] loss: 1.363\n",
            "[2,   240] loss: 1.371\n",
            "[2,   250] loss: 1.439\n",
            "[2,   260] loss: 1.399\n",
            "[2,   270] loss: 1.455\n",
            "[2,   280] loss: 1.398\n",
            "Validating\n",
            "[2,    10] loss: 1.404\n",
            "[2,    20] loss: 1.389\n",
            "[2,    30] loss: 1.391\n",
            "Training\n",
            "[3,    10] loss: 1.389\n",
            "[3,    20] loss: 1.377\n",
            "[3,    30] loss: 1.325\n",
            "[3,    40] loss: 1.336\n",
            "[3,    50] loss: 1.326\n",
            "[3,    60] loss: 1.275\n",
            "[3,    70] loss: 1.370\n",
            "[3,    80] loss: 1.337\n",
            "[3,    90] loss: 1.361\n",
            "[3,   100] loss: 1.301\n",
            "[3,   110] loss: 1.328\n",
            "[3,   120] loss: 1.307\n",
            "[3,   130] loss: 1.385\n",
            "[3,   140] loss: 1.373\n",
            "[3,   150] loss: 1.268\n",
            "[3,   160] loss: 1.344\n",
            "[3,   170] loss: 1.342\n",
            "[3,   180] loss: 1.285\n",
            "[3,   190] loss: 1.227\n",
            "[3,   200] loss: 1.272\n",
            "[3,   210] loss: 1.235\n",
            "[3,   220] loss: 1.308\n",
            "[3,   230] loss: 1.311\n",
            "[3,   240] loss: 1.235\n",
            "[3,   250] loss: 1.288\n",
            "[3,   260] loss: 1.311\n",
            "[3,   270] loss: 1.276\n",
            "[3,   280] loss: 1.316\n",
            "Validating\n",
            "[3,    10] loss: 1.269\n",
            "[3,    20] loss: 1.258\n",
            "[3,    30] loss: 1.300\n",
            "Training\n",
            "[4,    10] loss: 1.204\n",
            "[4,    20] loss: 1.268\n",
            "[4,    30] loss: 1.239\n",
            "[4,    40] loss: 1.196\n",
            "[4,    50] loss: 1.196\n",
            "[4,    60] loss: 1.157\n",
            "[4,    70] loss: 1.187\n",
            "[4,    80] loss: 1.179\n",
            "[4,    90] loss: 1.091\n",
            "[4,   100] loss: 1.151\n",
            "[4,   110] loss: 1.137\n",
            "[4,   120] loss: 1.156\n",
            "[4,   130] loss: 1.189\n",
            "[4,   140] loss: 1.149\n",
            "[4,   150] loss: 1.186\n",
            "[4,   160] loss: 1.194\n",
            "[4,   170] loss: 1.152\n",
            "[4,   180] loss: 1.104\n",
            "[4,   190] loss: 1.170\n",
            "[4,   200] loss: 1.180\n",
            "[4,   210] loss: 1.123\n",
            "[4,   220] loss: 1.205\n",
            "[4,   230] loss: 1.158\n",
            "[4,   240] loss: 1.149\n",
            "[4,   250] loss: 1.153\n",
            "[4,   260] loss: 1.139\n",
            "[4,   270] loss: 1.215\n",
            "[4,   280] loss: 1.139\n",
            "Validating\n",
            "[4,    10] loss: 1.324\n",
            "[4,    20] loss: 1.326\n",
            "[4,    30] loss: 1.366\n",
            "Training\n",
            "[5,    10] loss: 1.090\n",
            "[5,    20] loss: 1.032\n",
            "[5,    30] loss: 1.061\n",
            "[5,    40] loss: 1.020\n",
            "[5,    50] loss: 0.974\n",
            "[5,    60] loss: 0.970\n",
            "[5,    70] loss: 0.968\n",
            "[5,    80] loss: 1.037\n",
            "[5,    90] loss: 1.056\n",
            "[5,   100] loss: 1.047\n",
            "[5,   110] loss: 0.995\n",
            "[5,   120] loss: 0.994\n",
            "[5,   130] loss: 1.006\n",
            "[5,   140] loss: 1.000\n",
            "[5,   150] loss: 1.019\n",
            "[5,   160] loss: 1.009\n",
            "[5,   170] loss: 1.059\n",
            "[5,   180] loss: 1.081\n",
            "[5,   190] loss: 1.031\n",
            "[5,   200] loss: 1.091\n",
            "[5,   210] loss: 0.995\n",
            "[5,   220] loss: 1.055\n",
            "[5,   230] loss: 0.999\n",
            "[5,   240] loss: 1.087\n",
            "[5,   250] loss: 1.025\n",
            "[5,   260] loss: 1.091\n",
            "[5,   270] loss: 1.085\n",
            "[5,   280] loss: 0.982\n",
            "Validating\n",
            "[5,    10] loss: 1.248\n",
            "[5,    20] loss: 1.244\n",
            "[5,    30] loss: 1.267\n",
            "Training\n",
            "[6,    10] loss: 0.835\n",
            "[6,    20] loss: 0.786\n",
            "[6,    30] loss: 0.786\n",
            "[6,    40] loss: 0.713\n",
            "[6,    50] loss: 0.789\n",
            "[6,    60] loss: 0.811\n",
            "[6,    70] loss: 0.801\n",
            "[6,    80] loss: 0.890\n",
            "[6,    90] loss: 0.819\n",
            "[6,   100] loss: 0.744\n",
            "[6,   110] loss: 0.829\n",
            "[6,   120] loss: 0.846\n",
            "[6,   130] loss: 0.870\n",
            "[6,   140] loss: 0.898\n",
            "[6,   150] loss: 0.860\n",
            "[6,   160] loss: 0.817\n",
            "[6,   170] loss: 0.879\n",
            "[6,   180] loss: 0.878\n",
            "[6,   190] loss: 0.862\n",
            "[6,   200] loss: 0.917\n",
            "[6,   210] loss: 0.851\n",
            "[6,   220] loss: 0.898\n",
            "[6,   230] loss: 0.865\n",
            "[6,   240] loss: 0.913\n",
            "[6,   250] loss: 0.886\n",
            "[6,   260] loss: 0.877\n",
            "[6,   270] loss: 0.883\n",
            "[6,   280] loss: 0.870\n",
            "Validating\n",
            "[6,    10] loss: 1.304\n",
            "[6,    20] loss: 1.260\n",
            "[6,    30] loss: 1.253\n",
            "Training\n",
            "[7,    10] loss: 0.638\n",
            "[7,    20] loss: 0.618\n",
            "[7,    30] loss: 0.586\n",
            "[7,    40] loss: 0.596\n",
            "[7,    50] loss: 0.556\n",
            "[7,    60] loss: 0.537\n",
            "[7,    70] loss: 0.524\n",
            "[7,    80] loss: 0.611\n",
            "[7,    90] loss: 0.635\n",
            "[7,   100] loss: 0.594\n",
            "[7,   110] loss: 0.587\n",
            "[7,   120] loss: 0.609\n",
            "[7,   130] loss: 0.555\n",
            "[7,   140] loss: 0.651\n",
            "[7,   150] loss: 0.665\n",
            "[7,   160] loss: 0.630\n",
            "[7,   170] loss: 0.651\n",
            "[7,   180] loss: 0.709\n",
            "[7,   190] loss: 0.693\n",
            "[7,   200] loss: 0.709\n",
            "[7,   210] loss: 0.651\n",
            "[7,   220] loss: 0.685\n",
            "[7,   230] loss: 0.702\n",
            "[7,   240] loss: 0.692\n",
            "[7,   250] loss: 0.630\n",
            "[7,   260] loss: 0.675\n",
            "[7,   270] loss: 0.697\n",
            "[7,   280] loss: 0.691\n",
            "Validating\n",
            "[7,    10] loss: 1.334\n",
            "[7,    20] loss: 1.449\n",
            "[7,    30] loss: 1.481\n",
            "Training\n",
            "[8,    10] loss: 0.516\n",
            "[8,    20] loss: 0.353\n",
            "[8,    30] loss: 0.365\n",
            "[8,    40] loss: 0.398\n",
            "[8,    50] loss: 0.374\n",
            "[8,    60] loss: 0.360\n",
            "[8,    70] loss: 0.363\n",
            "[8,    80] loss: 0.378\n",
            "[8,    90] loss: 0.400\n",
            "[8,   100] loss: 0.452\n",
            "[8,   110] loss: 0.388\n",
            "[8,   120] loss: 0.405\n",
            "[8,   130] loss: 0.458\n",
            "[8,   140] loss: 0.511\n",
            "[8,   150] loss: 0.444\n",
            "[8,   160] loss: 0.427\n",
            "[8,   170] loss: 0.448\n",
            "[8,   180] loss: 0.429\n",
            "[8,   190] loss: 0.471\n",
            "[8,   200] loss: 0.526\n",
            "[8,   210] loss: 0.581\n",
            "[8,   220] loss: 0.466\n",
            "[8,   230] loss: 0.459\n",
            "[8,   240] loss: 0.476\n",
            "[8,   250] loss: 0.516\n",
            "[8,   260] loss: 0.441\n",
            "[8,   270] loss: 0.475\n",
            "[8,   280] loss: 0.503\n",
            "Validating\n",
            "[8,    10] loss: 1.571\n",
            "[8,    20] loss: 1.594\n",
            "[8,    30] loss: 1.472\n",
            "Training\n",
            "[9,    10] loss: 0.296\n",
            "[9,    20] loss: 0.245\n",
            "[9,    30] loss: 0.243\n",
            "[9,    40] loss: 0.295\n",
            "[9,    50] loss: 0.222\n",
            "[9,    60] loss: 0.236\n",
            "[9,    70] loss: 0.291\n",
            "[9,    80] loss: 0.270\n",
            "[9,    90] loss: 0.280\n",
            "[9,   100] loss: 0.236\n",
            "[9,   110] loss: 0.252\n",
            "[9,   120] loss: 0.274\n",
            "[9,   130] loss: 0.308\n",
            "[9,   140] loss: 0.272\n",
            "[9,   150] loss: 0.272\n",
            "[9,   160] loss: 0.299\n",
            "[9,   170] loss: 0.315\n",
            "[9,   180] loss: 0.332\n",
            "[9,   190] loss: 0.292\n",
            "[9,   200] loss: 0.317\n",
            "[9,   210] loss: 0.363\n",
            "[9,   220] loss: 0.305\n",
            "[9,   230] loss: 0.294\n",
            "[9,   240] loss: 0.317\n",
            "[9,   250] loss: 0.370\n",
            "[9,   260] loss: 0.358\n",
            "[9,   270] loss: 0.443\n",
            "[9,   280] loss: 0.394\n",
            "Validating\n",
            "[9,    10] loss: 1.905\n",
            "[9,    20] loss: 1.714\n",
            "[9,    30] loss: 1.845\n",
            "Training\n",
            "[10,    10] loss: 0.218\n",
            "[10,    20] loss: 0.215\n",
            "[10,    30] loss: 0.239\n",
            "[10,    40] loss: 0.213\n",
            "[10,    50] loss: 0.193\n",
            "[10,    60] loss: 0.234\n",
            "[10,    70] loss: 0.172\n",
            "[10,    80] loss: 0.193\n",
            "[10,    90] loss: 0.236\n",
            "[10,   100] loss: 0.262\n",
            "[10,   110] loss: 0.189\n",
            "[10,   120] loss: 0.178\n",
            "[10,   130] loss: 0.210\n",
            "[10,   140] loss: 0.209\n",
            "[10,   150] loss: 0.223\n",
            "[10,   160] loss: 0.245\n",
            "[10,   170] loss: 0.168\n",
            "[10,   180] loss: 0.223\n",
            "[10,   190] loss: 0.219\n",
            "[10,   200] loss: 0.178\n",
            "[10,   210] loss: 0.265\n",
            "[10,   220] loss: 0.223\n",
            "[10,   230] loss: 0.249\n",
            "[10,   240] loss: 0.239\n",
            "[10,   250] loss: 0.256\n",
            "[10,   260] loss: 0.191\n",
            "[10,   270] loss: 0.278\n",
            "[10,   280] loss: 0.323\n",
            "Validating\n",
            "[10,    10] loss: 2.009\n",
            "[10,    20] loss: 1.928\n",
            "[10,    30] loss: 2.185\n",
            "Training\n",
            "[11,    10] loss: 0.153\n",
            "[11,    20] loss: 0.156\n",
            "[11,    30] loss: 0.156\n",
            "[11,    40] loss: 0.107\n",
            "[11,    50] loss: 0.097\n",
            "[11,    60] loss: 0.117\n",
            "[11,    70] loss: 0.125\n",
            "[11,    80] loss: 0.120\n",
            "[11,    90] loss: 0.168\n",
            "[11,   100] loss: 0.157\n",
            "[11,   110] loss: 0.146\n",
            "[11,   120] loss: 0.134\n",
            "[11,   130] loss: 0.201\n",
            "[11,   140] loss: 0.200\n",
            "[11,   150] loss: 0.183\n",
            "[11,   160] loss: 0.189\n",
            "[11,   170] loss: 0.228\n",
            "[11,   180] loss: 0.211\n",
            "[11,   190] loss: 0.138\n",
            "[11,   200] loss: 0.136\n",
            "[11,   210] loss: 0.151\n",
            "[11,   220] loss: 0.181\n",
            "[11,   230] loss: 0.162\n",
            "[11,   240] loss: 0.193\n",
            "[11,   250] loss: 0.192\n",
            "[11,   260] loss: 0.224\n",
            "[11,   270] loss: 0.179\n",
            "[11,   280] loss: 0.214\n",
            "Validating\n",
            "[11,    10] loss: 1.954\n",
            "[11,    20] loss: 1.904\n",
            "[11,    30] loss: 1.884\n",
            "Training\n",
            "[12,    10] loss: 0.122\n",
            "[12,    20] loss: 0.135\n",
            "[12,    30] loss: 0.135\n",
            "[12,    40] loss: 0.121\n",
            "[12,    50] loss: 0.122\n",
            "[12,    60] loss: 0.134\n",
            "[12,    70] loss: 0.089\n",
            "[12,    80] loss: 0.095\n",
            "[12,    90] loss: 0.148\n",
            "[12,   100] loss: 0.112\n",
            "[12,   110] loss: 0.102\n",
            "[12,   120] loss: 0.118\n",
            "[12,   130] loss: 0.194\n",
            "[12,   140] loss: 0.128\n",
            "[12,   150] loss: 0.131\n",
            "[12,   160] loss: 0.133\n",
            "[12,   170] loss: 0.137\n",
            "[12,   180] loss: 0.124\n",
            "[12,   190] loss: 0.128\n",
            "[12,   200] loss: 0.102\n",
            "[12,   210] loss: 0.117\n",
            "[12,   220] loss: 0.107\n",
            "[12,   230] loss: 0.135\n",
            "[12,   240] loss: 0.121\n",
            "[12,   250] loss: 0.157\n",
            "[12,   260] loss: 0.177\n",
            "[12,   270] loss: 0.184\n",
            "[12,   280] loss: 0.197\n",
            "Validating\n",
            "[12,    10] loss: 2.379\n",
            "[12,    20] loss: 2.046\n",
            "[12,    30] loss: 1.970\n",
            "Training\n",
            "[13,    10] loss: 0.114\n",
            "[13,    20] loss: 0.107\n",
            "[13,    30] loss: 0.102\n",
            "[13,    40] loss: 0.105\n",
            "[13,    50] loss: 0.093\n",
            "[13,    60] loss: 0.108\n",
            "[13,    70] loss: 0.076\n",
            "[13,    80] loss: 0.085\n",
            "[13,    90] loss: 0.100\n",
            "[13,   100] loss: 0.091\n",
            "[13,   110] loss: 0.089\n",
            "[13,   120] loss: 0.102\n",
            "[13,   130] loss: 0.151\n",
            "[13,   140] loss: 0.112\n",
            "[13,   150] loss: 0.107\n",
            "[13,   160] loss: 0.120\n",
            "[13,   170] loss: 0.087\n",
            "[13,   180] loss: 0.139\n",
            "[13,   190] loss: 0.123\n",
            "[13,   200] loss: 0.134\n",
            "[13,   210] loss: 0.102\n",
            "[13,   220] loss: 0.103\n",
            "[13,   230] loss: 0.094\n",
            "[13,   240] loss: 0.121\n",
            "[13,   250] loss: 0.119\n",
            "[13,   260] loss: 0.119\n",
            "[13,   270] loss: 0.113\n",
            "[13,   280] loss: 0.132\n",
            "Validating\n",
            "[13,    10] loss: 2.248\n",
            "[13,    20] loss: 2.437\n",
            "[13,    30] loss: 2.266\n",
            "Training\n",
            "[14,    10] loss: 0.516\n",
            "[14,    20] loss: 0.274\n",
            "[14,    30] loss: 0.185\n",
            "[14,    40] loss: 0.168\n",
            "[14,    50] loss: 0.159\n",
            "[14,    60] loss: 0.181\n",
            "[14,    70] loss: 0.149\n",
            "[14,    80] loss: 0.168\n",
            "[14,    90] loss: 0.174\n",
            "[14,   100] loss: 0.172\n",
            "[14,   110] loss: 0.146\n",
            "[14,   120] loss: 0.153\n",
            "[14,   130] loss: 0.182\n",
            "[14,   140] loss: 0.148\n",
            "[14,   150] loss: 0.117\n",
            "[14,   160] loss: 0.128\n",
            "[14,   170] loss: 0.147\n",
            "[14,   180] loss: 0.131\n",
            "[14,   190] loss: 0.152\n",
            "[14,   200] loss: 0.142\n",
            "[14,   210] loss: 0.143\n",
            "[14,   220] loss: 0.182\n",
            "[14,   230] loss: 0.156\n",
            "[14,   240] loss: 0.092\n",
            "[14,   250] loss: 0.143\n",
            "[14,   260] loss: 0.122\n",
            "[14,   270] loss: 0.176\n",
            "[14,   280] loss: 0.153\n",
            "Validating\n",
            "[14,    10] loss: 2.211\n",
            "[14,    20] loss: 2.111\n",
            "[14,    30] loss: 2.227\n",
            "Training\n",
            "[15,    10] loss: 0.189\n",
            "[15,    20] loss: 0.226\n",
            "[15,    30] loss: 0.195\n",
            "[15,    40] loss: 0.154\n",
            "[15,    50] loss: 0.104\n",
            "[15,    60] loss: 0.115\n",
            "[15,    70] loss: 0.102\n",
            "[15,    80] loss: 0.096\n",
            "[15,    90] loss: 0.084\n",
            "[15,   100] loss: 0.107\n",
            "[15,   110] loss: 0.115\n",
            "[15,   120] loss: 0.128\n",
            "[15,   130] loss: 0.099\n",
            "[15,   140] loss: 0.123\n",
            "[15,   150] loss: 0.087\n",
            "[15,   160] loss: 0.062\n",
            "[15,   170] loss: 0.080\n",
            "[15,   180] loss: 0.086\n",
            "[15,   190] loss: 0.073\n",
            "[15,   200] loss: 0.116\n",
            "[15,   210] loss: 0.126\n",
            "[15,   220] loss: 0.132\n",
            "[15,   230] loss: 0.141\n",
            "[15,   240] loss: 0.111\n",
            "[15,   250] loss: 0.116\n",
            "[15,   260] loss: 0.100\n",
            "[15,   270] loss: 0.110\n",
            "[15,   280] loss: 0.101\n",
            "Validating\n",
            "[15,    10] loss: 2.733\n",
            "[15,    20] loss: 2.388\n",
            "[15,    30] loss: 2.279\n",
            "Training\n",
            "[16,    10] loss: 0.059\n",
            "[16,    20] loss: 0.040\n",
            "[16,    30] loss: 0.067\n",
            "[16,    40] loss: 0.067\n",
            "[16,    50] loss: 0.045\n",
            "[16,    60] loss: 0.044\n",
            "[16,    70] loss: 0.073\n",
            "[16,    80] loss: 0.086\n",
            "[16,    90] loss: 0.070\n",
            "[16,   100] loss: 0.039\n",
            "[16,   110] loss: 0.058\n",
            "[16,   120] loss: 0.062\n",
            "[16,   130] loss: 0.067\n",
            "[16,   140] loss: 0.067\n",
            "[16,   150] loss: 0.080\n",
            "[16,   160] loss: 0.083\n",
            "[16,   170] loss: 0.059\n",
            "[16,   180] loss: 0.089\n",
            "[16,   190] loss: 0.069\n",
            "[16,   200] loss: 0.080\n",
            "[16,   210] loss: 0.105\n",
            "[16,   220] loss: 0.102\n",
            "[16,   230] loss: 0.107\n",
            "[16,   240] loss: 0.066\n",
            "[16,   250] loss: 0.111\n",
            "[16,   260] loss: 0.149\n",
            "[16,   270] loss: 0.116\n",
            "[16,   280] loss: 0.119\n",
            "Validating\n",
            "[16,    10] loss: 2.417\n",
            "[16,    20] loss: 2.563\n",
            "[16,    30] loss: 2.401\n",
            "Training\n",
            "[17,    10] loss: 0.073\n",
            "[17,    20] loss: 0.042\n",
            "[17,    30] loss: 0.063\n",
            "[17,    40] loss: 0.078\n",
            "[17,    50] loss: 0.035\n",
            "[17,    60] loss: 0.049\n",
            "[17,    70] loss: 0.073\n",
            "[17,    80] loss: 0.076\n",
            "[17,    90] loss: 0.074\n",
            "[17,   100] loss: 0.056\n",
            "[17,   110] loss: 0.046\n",
            "[17,   120] loss: 0.063\n",
            "[17,   130] loss: 0.039\n",
            "[17,   140] loss: 0.053\n",
            "[17,   150] loss: 0.060\n",
            "[17,   160] loss: 0.077\n",
            "[17,   170] loss: 0.091\n",
            "[17,   180] loss: 0.062\n",
            "[17,   190] loss: 0.069\n",
            "[17,   200] loss: 0.082\n",
            "[17,   210] loss: 0.064\n",
            "[17,   220] loss: 0.050\n",
            "[17,   230] loss: 0.051\n",
            "[17,   240] loss: 0.061\n",
            "[17,   250] loss: 0.059\n",
            "[17,   260] loss: 0.054\n",
            "[17,   270] loss: 0.065\n",
            "[17,   280] loss: 0.063\n",
            "Validating\n",
            "[17,    10] loss: 3.024\n",
            "[17,    20] loss: 3.262\n",
            "[17,    30] loss: 3.005\n",
            "Training\n",
            "[18,    10] loss: 0.032\n",
            "[18,    20] loss: 0.048\n",
            "[18,    30] loss: 0.045\n",
            "[18,    40] loss: 0.039\n",
            "[18,    50] loss: 0.031\n",
            "[18,    60] loss: 0.044\n",
            "[18,    70] loss: 0.049\n",
            "[18,    80] loss: 0.045\n",
            "[18,    90] loss: 0.053\n",
            "[18,   100] loss: 0.042\n",
            "[18,   110] loss: 0.029\n",
            "[18,   120] loss: 0.041\n",
            "[18,   130] loss: 0.065\n",
            "[18,   140] loss: 0.023\n",
            "[18,   150] loss: 0.044\n",
            "[18,   160] loss: 0.043\n",
            "[18,   170] loss: 0.030\n",
            "[18,   180] loss: 0.051\n",
            "[18,   190] loss: 0.037\n",
            "[18,   200] loss: 0.071\n",
            "[18,   210] loss: 0.032\n",
            "[18,   220] loss: 0.054\n",
            "[18,   230] loss: 0.046\n",
            "[18,   240] loss: 0.028\n",
            "[18,   250] loss: 0.056\n",
            "[18,   260] loss: 0.039\n",
            "[18,   270] loss: 0.040\n",
            "[18,   280] loss: 0.053\n",
            "Validating\n",
            "[18,    10] loss: 2.699\n",
            "[18,    20] loss: 2.809\n",
            "[18,    30] loss: 2.603\n",
            "Training\n",
            "[19,    10] loss: 0.027\n",
            "[19,    20] loss: 0.026\n",
            "[19,    30] loss: 0.030\n",
            "[19,    40] loss: 0.033\n",
            "[19,    50] loss: 0.013\n",
            "[19,    60] loss: 0.023\n",
            "[19,    70] loss: 0.020\n",
            "[19,    80] loss: 0.023\n",
            "[19,    90] loss: 0.036\n",
            "[19,   100] loss: 0.040\n",
            "[19,   110] loss: 0.032\n",
            "[19,   120] loss: 0.049\n",
            "[19,   130] loss: 0.028\n",
            "[19,   140] loss: 0.053\n",
            "[19,   150] loss: 0.029\n",
            "[19,   160] loss: 0.040\n",
            "[19,   170] loss: 0.045\n",
            "[19,   180] loss: 0.043\n",
            "[19,   190] loss: 0.036\n",
            "[19,   200] loss: 0.045\n",
            "[19,   210] loss: 0.035\n",
            "[19,   220] loss: 0.030\n",
            "[19,   230] loss: 0.025\n",
            "[19,   240] loss: 0.037\n",
            "[19,   250] loss: 0.037\n",
            "[19,   260] loss: 0.036\n",
            "[19,   270] loss: 0.032\n",
            "[19,   280] loss: 0.042\n",
            "Validating\n",
            "[19,    10] loss: 3.060\n",
            "[19,    20] loss: 3.144\n",
            "[19,    30] loss: 2.870\n",
            "Training\n",
            "[20,    10] loss: 0.022\n",
            "[20,    20] loss: 0.036\n",
            "[20,    30] loss: 0.032\n",
            "[20,    40] loss: 0.019\n",
            "[20,    50] loss: 0.016\n",
            "[20,    60] loss: 0.022\n",
            "[20,    70] loss: 0.025\n",
            "[20,    80] loss: 0.030\n",
            "[20,    90] loss: 0.027\n",
            "[20,   100] loss: 0.022\n",
            "[20,   110] loss: 0.026\n",
            "[20,   120] loss: 0.043\n",
            "[20,   130] loss: 0.024\n",
            "[20,   140] loss: 0.052\n",
            "[20,   150] loss: 0.029\n",
            "[20,   160] loss: 0.030\n",
            "[20,   170] loss: 0.046\n",
            "[20,   180] loss: 0.026\n",
            "[20,   190] loss: 0.048\n",
            "[20,   200] loss: 0.032\n",
            "[20,   210] loss: 0.037\n",
            "[20,   220] loss: 0.079\n",
            "[20,   230] loss: 0.041\n",
            "[20,   240] loss: 0.027\n",
            "[20,   250] loss: 0.032\n",
            "[20,   260] loss: 0.031\n",
            "[20,   270] loss: 0.027\n",
            "[20,   280] loss: 0.025\n",
            "Validating\n",
            "[20,    10] loss: 3.238\n",
            "[20,    20] loss: 2.849\n",
            "[20,    30] loss: 2.955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaGNnlXTJdeG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}