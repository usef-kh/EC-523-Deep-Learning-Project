{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE FOR GETTING INPUT TO THE ELM \n",
    "import numpy as np\n",
    "from sklearn.svm import SVC # support vector classifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class CNN_2D_features(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=7, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=2)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=2)\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 2 * 4, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        #self.fc3 = nn.Linear(4096, 7)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv1(x))\n",
    "        # print(x.shape)\n",
    "        x = self.max_pool(x)\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.avg_pool(x)\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.avg_pool(x)\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv4(x))\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 512 * 2 * 4)\n",
    "        # print(x.shape)\n",
    "        x = self.drop(x)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "\n",
    "        #x = F.elu(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN_3D_features(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1a = nn.Conv3d(3, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.conv2a = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "\n",
    "        self.conv3a = nn.Conv2d(128, 256, kernel_size=3, stride=1)\n",
    "        self.conv3b = nn.Conv2d(256, 256, kernel_size=3, stride=1)\n",
    "\n",
    "        self.conv4a = nn.Conv2d(256, 512, kernel_size=3, stride=1)\n",
    "        self.conv4b = nn.Conv2d(512, 512, kernel_size=3, stride=1)\n",
    "\n",
    "        self.conv5a = nn.Conv2d(512, 512, kernel_size=3, stride=1)\n",
    "        self.conv5b = nn.Conv2d(512, 512, kernel_size=3, stride=1)\n",
    "\n",
    "        self.max_pool = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=2)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(100, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        #self.fc3 = nn.Linear(4096, 7)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = x.view(-1, 100)\n",
    "\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "\n",
    "        #x = F.elu(self.fc3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_2D(CNN_2D_features):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc3 = nn.Linear(4096, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        x = F.elu(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN_3D(CNN_3D_features):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc3 = nn.Linear(4096, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        x = F.elu(self.fc3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to first ELM:\n",
    "# input size will be the concatenated length of the output of the two CNNs\n",
    "class ELM_features(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_classes, device=None):\n",
    "    super().__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = num_classes # 2 for first ELM, 5 for second\n",
    "    self.device = device\n",
    "\n",
    "    # just two layers; declare parameters: (maybe use xavier_uniform inits?) \n",
    "    self.alpha = nn.Linear(input_size, hidden_size)\n",
    "    \n",
    "    #self.alpha = nn.init.uniform_(torch.empty(self.input_size, self.hidden_size, device=self.device), a=-1., b=1.)\n",
    "    #self.beta = nn.init.uniform_(torch.empty(self.hidden_size, self.output_size, device=self.device), a=-1., b=1.)\n",
    "\n",
    "    #self.bias = torch.zeros(self.hidden_size, device=self.device)\n",
    "    # bias included in alpha\n",
    "\n",
    "    #self.activation = torch.nn.functional.gelu # other activations? # they said 'gaussian kernel'\n",
    "\n",
    "  def forward(self, x):\n",
    "    h = torch.nn.functional.gelu(self.alpha(x)) # forward used for training\n",
    "    #return x.mm(beta)\n",
    "    return h\n",
    "\n",
    "  #def forwardToHidden(self, x):  # the output of this is what we feed to the next ELM AFTER THIS ONE IS TRAINED\n",
    "  #  return self.activation(torch.add(x.mm(self.alpha), self.bias))\n",
    "\n",
    "# Input to first ELM:\n",
    "# input size will be the concatenated length of the output of the two CNNs\n",
    "# NOT USING THIS FUNCTION CURRENTLY\n",
    "class ELM(ELM_features):\n",
    "  def __init__(self, input_size, hidden_size, num_classes, device=None):\n",
    "    super().__init__(input_size, hidden_size, num_classes, device)\n",
    "\n",
    "    self.beta = nn.Linear(hidden_size, num_classes, bias=False)\n",
    "    #self.beta = nn.init.uniform_(torch.empty(self.hidden_size, self.output_size, device=self.device), a=-1., b=1.)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = super().forward(x)\n",
    "    #h = self.activation(torch.add(x.mm(self.alpha), self.bias)) # forward used for training\n",
    "    return self.beta(x)\n",
    "    #return h\n",
    "\n",
    "  def forwardToHidden(self, x):  # the output of this is what we feed to the next ELM AFTER THIS ONE IS TRAINED\n",
    "    return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(model, params):\n",
    "    \"\"\" Load params into all layers of 'model'\n",
    "        that are compatible, then freeze them\"\"\"\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    imp_params = {k: v for k, v in params.items() if k in model_dict}\n",
    "\n",
    "    # Load layers\n",
    "    model_dict.update(imp_params)\n",
    "    model.load_state_dict(imp_params)\n",
    "\n",
    "    # Freeze layers\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of CNN_2D_features, CNN_3D_features\n",
    "CNN_2D_feat = CNN_2D_features()\n",
    "CNN_3D_feat = CNN_3D_features()\n",
    "\n",
    "#################\n",
    "# DEFINE PARAMS #\n",
    "# params_2D = \n",
    "# params_3D = \n",
    "#################\n",
    "\n",
    "# Load weights from trained CNN_2D, CNN_3D into newly initialized CNN_2D_feat, CNN_3D_feat\n",
    "load_features(CNN_2D_feat, params_2D)\n",
    "load_features(CNN_3D_feat, params_3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data through CNNs, concatenate outputs (flatten) (1x4096 + 1x4096 = 1x8192), this is the input to the ELM \n",
    "# input will be Bx8192 (B = batch size)\n",
    "## TRAIN FIRST ELM ##\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "# Input to first ELM:\n",
    "# input size will be the concatenated length of the output of the two CNNs\n",
    "class CNN_ELM():\n",
    "  def __init__(self, input_size, hidden_size, num_classes, CNN_2D, CNN_3D, ELM, device=None):\n",
    "    \n",
    "    self.CNN_2D = CNN_2D\n",
    "    self.CNN_3D = CNN_3D\n",
    "    \n",
    "    self.ELM_feat = ELM_features(input_size, hidden_size, num_classes, device)\n",
    "    #self.ELM = ELM(input_size, hidden_size, num_classes, device)\n",
    "    \n",
    "    self.beta = nn.init.uniform_(torch.empty(self.hidden_size, self.output_size, device=self.device), a=-1., b=1.)\n",
    "\n",
    "  def forward(self, audio, video):\n",
    "    x_aud = self.CNN_2D(audio)\n",
    "    x_vid = self.CNN_3D(video)\n",
    "    \n",
    "    x = torch.cat((x_aud, x_vid), dim=1)\n",
    "    \n",
    "    x = self.ELM_feat(x)\n",
    "    \n",
    "    return x.mm(beta)\n",
    "    #return h\n",
    "\n",
    "  def forwardToHidden(self, x):  # the output of this is what we feed to the next ELM AFTER THIS ONE IS TRAINED\n",
    "    x_aud = self.CNN_2D(audio)\n",
    "    x_vid = self.CNN_3D(video)\n",
    "    \n",
    "    x = torch.cat((x_aud, x_vid), dim=1)\n",
    "    \n",
    "    x = self.ELM_feat(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pseudoInverse(object):\n",
    "    def __init__(self,params,C=1e-2,forgettingfactor=1,L =100):\n",
    "        self.params=list(params)\n",
    "        self.is_cuda=False #self.params[len(self.params)-1].is_cuda\n",
    "        self.C=C\n",
    "        self.L=L\n",
    "        self.w=self.params[len(self.params)-1]\n",
    "        self.w.data.fill_(0) #initialize output weight as zeros\n",
    "        # For sequential learning in OS-ELM\n",
    "        self.dimInput=self.params[len(self.params)-1].data.size()[1]\n",
    "        self.forgettingfactor=forgettingfactor\n",
    "        self.M=Variable(torch.inverse(self.C*torch.eye(self.dimInput)),requires_grad=False, volatile=True)\n",
    "\n",
    "        if self.is_cuda:\n",
    "            self.M=self.M.cuda()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.M = Variable(torch.inverse(self.C * torch.eye(self.dimInput)),requires_grad=False, volatile=True)\n",
    "\n",
    "        if self.is_cuda:\n",
    "            self.M = self.M.cuda()\n",
    "        self.w = self.params[len(self.params) - 1]\n",
    "        self.w.data.fill_(0.0)\n",
    "\n",
    "    def pseudoBig(self,inputs,oneHotTarget):\n",
    "        #print(inputs.shape)\n",
    "        xtx = torch.mm(inputs.t(), inputs) # [ n_features * n_features ]\n",
    "        dimInput=inputs.size()[1]\n",
    "        I = Variable(torch.eye(dimInput),requires_grad=False, volatile=True)\n",
    "        if self.is_cuda:\n",
    "            I = I.cuda()\n",
    "        if self.L > 0.0:\n",
    "            mu = torch.mean(inputs, dim=0, keepdim=True)  # [ 1 * n_features ]\n",
    "            S = inputs - mu\n",
    "            S = torch.mm(S.t(), S)\n",
    "            self.M = Variable(torch.inverse(xtx.data + self.C * (I.data+self.L*S.data)),requires_grad=False, volatile=True)\n",
    "        else:\n",
    "            self.M = Variable(torch.inverse(xtx.data + self.C *I.data), requires_grad=False, volatile=True)\n",
    "\n",
    "        w = torch.mm(self.M, inputs.t())\n",
    "        w = torch.mm(w, oneHotTarget)\n",
    "        self.w.data = w.t().data\n",
    "\n",
    "    def pseudoSmall(self,inputs,oneHotTarget):\n",
    "        xxt = torch.mm(inputs, inputs.t())\n",
    "        numSamples=inputs.size()[0]\n",
    "        I = Variable(torch.eye(numSamples),requires_grad=False, volatile=True)\n",
    "        if self.is_cuda:\n",
    "            I = I.cuda()\n",
    "        self.M = Variable(torch.inverse(xxt.data + self.C * I.data),requires_grad=False, volatile=True)\n",
    "        w = torch.mm(inputs.t(), self.M)\n",
    "        w = torch.mm(w, oneHotTarget)\n",
    "\n",
    "        self.w.data = w.t().data\n",
    "\n",
    "    def train(self,inputs,targets, oneHotVectorize=True):\n",
    "        #targets = targets.view(targets.size(0),-1)\n",
    "        #print(\"targets:\", targets)\n",
    "        if oneHotVectorize:\n",
    "            targets=self.oneHotVectorize(targets=targets)\n",
    "        numSamples=inputs.size()[0]\n",
    "        dimInput=inputs.size()[1]\n",
    "        dimTarget=targets.size()[1]\n",
    "\n",
    "        if numSamples>dimInput:\n",
    "            self.pseudoBig(inputs,targets)\n",
    "        else:\n",
    "            self.pseudoSmall(inputs,targets)\n",
    "\n",
    "\n",
    "\n",
    "    def train_sequential(self,inputs,targets):\n",
    "        oneHotTarget = self.oneHotVectorize(targets=targets)\n",
    "        numSamples = inputs.size()[0]\n",
    "        dimInput = inputs.size()[1]\n",
    "        dimTarget = oneHotTarget.size()[1]\n",
    "\n",
    "        if numSamples<dimInput:\n",
    "            I1 = Variable(torch.eye(dimInput))\n",
    "            if self.is_cuda:\n",
    "                I1 = I1.cuda()\n",
    "            xtx=torch.mm(inputs.t(),inputs)\n",
    "            self.M=Variable(torch.inverse(xtx.data+self.C*I1.data),requires_grad=False, volatile=True)\n",
    "\n",
    "        I = Variable(torch.eye(numSamples))\n",
    "        if self.is_cuda:\n",
    "            I = I.cuda()\n",
    "\n",
    "        self.M = (1/self.forgettingfactor) * self.M - torch.mm((1/self.forgettingfactor) * self.M,\n",
    "                                             torch.mm(inputs.t(), torch.mm(Variable(torch.inverse(I.data + torch.mm(inputs, torch.mm((1/self.forgettingfactor)* self.M, inputs.t())).data),requires_grad=False, volatile=True),\n",
    "                                             torch.mm(inputs, (1/self.forgettingfactor)* self.M))))\n",
    "\n",
    "\n",
    "        self.w.data += torch.mm(self.M,torch.mm(inputs.t(),oneHotTarget - torch.mm(inputs,self.w.t()))).t().data\n",
    "\n",
    "\n",
    "    def oneHotVectorize(self,targets):\n",
    "        oneHotTarget=torch.zeros(targets.size()[0],targets.max()+1)\n",
    "\n",
    "        for i in range(targets.size()[0]):\n",
    "            oneHotTarget[i][targets[i]]=1\n",
    "\n",
    "        if self.is_cuda:\n",
    "            oneHotTarget=oneHotTarget.cuda()\n",
    "        oneHotTarget=Variable(oneHotTarget,requires_grad=False, volatile=True)\n",
    "\n",
    "        return oneHotTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/python3/3.6.9/install/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if sys.path[0] == '':\n",
      "/share/pkg.7/python3/3.6.9/install/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/share/pkg.7/python3/3.6.9/install/lib/python3.6/site-packages/ipykernel_launcher.py:105: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/share/pkg.7/python3/3.6.9/install/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/share/pkg.7/python3/3.6.9/install/lib/python3.6/site-packages/ipykernel_launcher.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:10000/10000 (100.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/python3/3.6.9/install/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 4984/10000 (49.84%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST FIRST MODEL \n",
    "from torch.autograd import Variable\n",
    "#############################\n",
    "# LOAD AUDIO AND VIDEO DATA #\n",
    "#############################\n",
    "# random data\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "n = 10000\n",
    "bs = 32\n",
    "X = torch.rand(n, 8192)\n",
    "Y = torch.randint(0, 2, (n,))\n",
    "samples = []\n",
    "for x, y in zip(X, Y):\n",
    "    samples.append((x, y.item()))\n",
    "\n",
    "train_loader = DataLoader(samples, batch_size=bs, shuffle=True)\n",
    "\n",
    "\n",
    "X_t = torch.rand(n, 8192)\n",
    "Y_t = torch.randint(0, 2, (n,))\n",
    "t_samples = []\n",
    "for x, y in zip(X_t, Y_t):\n",
    "    t_samples.append((x, y.item()))\n",
    "\n",
    "test_loader = DataLoader(t_samples, batch_size=bs, shuffle=True)\n",
    "\n",
    "# for sample in train_loader:\n",
    "#     print(sample)\n",
    "\n",
    "# Steps:\n",
    "'''\n",
    "1) Train first model\n",
    "2) Get outputs of first model\n",
    "3) train second model on hidden layer output of first model\n",
    "4) pass hidden layer output of second model to SVM\n",
    "'''\n",
    "### TRAINING FIRST MODEL ####\n",
    "num_classes = 2\n",
    "input_size = 8192\n",
    "hidden_size = 100\n",
    "#model = CNN_ELM(input_size, hidden_size, num_classes, CNN_2D_feat, CNN_3D_feat, ELM, device=None)\n",
    "\n",
    "# JUST TEST ELM #\n",
    "model = ELM(input_size, hidden_size, num_classes, device=None)\n",
    "cuda_avail = False\n",
    "if cuda_avail:\n",
    "  model.cuda()\n",
    "#print(list(model.parameters()))\n",
    "optimizer = pseudoInverse(params=model.parameters(), C=0.001, L=0)\n",
    "def train_ELM(model, optimizer, train_loader):\n",
    "  model.train()\n",
    "  correct=0\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    if cuda_avail:\n",
    "      data, target = data.cuda(), taget.cuda() # .device() \n",
    "    data, target = Variable(data, requires_grad = False, volatile=True),  Variable(target, requires_grad = False, volatile=True)\n",
    "    hiddenOut = model.forwardToHidden(data)\n",
    "    optimizer.train(inputs=hiddenOut, targets=target)\n",
    "    output=model.forward(data)\n",
    "    pred=output.data.max(1)[1]\n",
    "    correct+=pred.eq(target.data).cpu().sum()\n",
    "  print('Accuracy:{}/{} ({:.2f}%)\\n'.format(\n",
    "        correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "\n",
    "def test(model,test_loader):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda_avail:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data,requires_grad=False, volatile=True), Variable(target,requires_grad=False, volatile=True)\n",
    "        output = model.forward(data)\n",
    "        pred=output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "    print('\\nTest set accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "train_ELM(model,optimizer,train_loader)\n",
    "test(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def train_ELM(model, optimizer, train_loader):\\n  model.train()\\n  correct=0\\n  for batch_idx, (data, target) in enumerate(train_loader):\\n    if cuda_avail:\\n      data, target = data.cuda(), taget.cuda() # .device() \\n    data, target = Variable(data, requires_grad = False, volatile=True),  Variable(target, requires_grad = False, volatile=True)\\n    hiddenOut = model.forwardToHidden(data)\\n    optimizer.train(inputs=hiddenOut, targets=target)\\n    output=model.forward(data)\\n    pred=output.data.max(1)[1]\\n    correct+=pred.eq(target.data).cpu().sum()\\n  print('Accuracy:{}/{} ({:.2f}%)\\n'.format(\\n        correct, len(train_loader.dataset),\\n        100. * correct / len(train_loader.dataset)))\\n  \\ndef test(model,test_loader):\\n    model.train()\\n    correct = 0\\n    for data, target in test_loader:\\n        if cuda_avail:\\n            data, target = data.cuda(), target.cuda()\\n        data, target = Variable(data,requires_grad=False, volatile=True), Variable(target,requires_grad=False, volatile=True)\\n        output = model.forward(data)\\n        pred=output.data.max(1)[1]\\n        correct += pred.eq(target.data).cpu().sum()\\n    print('\\nTest set accuracy: {}/{} ({:.2f}%)\\n'.format(\\n        correct, len(test_loader.dataset),\\n        100. * correct / len(test_loader.dataset)))\\n    \\ntrain(args,model,optimizer,train_loader)\\ntest(args,model,test_loader)\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def train_ELM(model, optimizer, train_loader):\n",
    "  model.train()\n",
    "  correct=0\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    if cuda_avail:\n",
    "      data, target = data.cuda(), taget.cuda() # .device() \n",
    "    data, target = Variable(data, requires_grad = False, volatile=True),  Variable(target, requires_grad = False, volatile=True)\n",
    "    hiddenOut = model.forwardToHidden(data)\n",
    "    optimizer.train(inputs=hiddenOut, targets=target)\n",
    "    output=model.forward(data)\n",
    "    pred=output.data.max(1)[1]\n",
    "    correct+=pred.eq(target.data).cpu().sum()\n",
    "  print('Accuracy:{}/{} ({:.2f}%)\\n'.format(\n",
    "        correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "  \n",
    "def test(model,test_loader):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda_avail:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data,requires_grad=False, volatile=True), Variable(target,requires_grad=False, volatile=True)\n",
    "        output = model.forward(data)\n",
    "        pred=output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "    print('\\nTest set accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "train(args,model,optimizer,train_loader)\n",
    "test(args,model,test_loader)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
